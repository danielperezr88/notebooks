{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Wholesale customers Data Set \n",
    "### https://archive.ics.uci.edu/ml/datasets/Wholesale+customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos referentes a perfil de gastos de un supermercado lisboeta. Resumen: El conjunto de datos se refiere a los clientes de un distribuidor al por mayor. Incluye el gasto anual en unidades monetarias (m.u.) en diversas categorías de productos.\n",
    "USOS: Predictor de perfil de consumo, Sistema recomendador.\n",
    "\n",
    "Wholesale_customers_data.csv\n",
    "\n",
    "campos: Channel,Region,Fresh,Milk,Grocery,Frozen,Detergents_Paper,Delicatessen\n",
    "\n",
    "En este caso la categoria podria ser cualquier de las diferentes categorias de consumo  \n",
    "1) FRESH: gasto anual (m.u.) en productos frescos (continua);\n",
    "2) leche: el gasto anual (m.u.) en los productos lácteos (continua);\n",
    "3) comestibles: el gasto anual (m.u.) en los productos comestibles (continua);\n",
    "4) Congelado: gasto anual (m.u.) en productos congelados (continua)\n",
    "5) DETERGENTS_PAPER: gasto anual (m.u.) en detergentes y productos de papel (continua)\n",
    "6) Delicatessen: gasto anual (m.u.) y en productos delicatessen (continuos);\n",
    "7) Canal: Canal customersâ € ™ - Horeca (Hotel / Restaurante / Café ©) o canal de venta al por menor (nominal)\n",
    "8) REGIÓN: customersâ € ™ â € Región \"Lisnon, Oporto o Otros (nominal)\n",
    "\n",
    "\n",
    "Elegimos: delicatessen como categoria, ya que el supermercado está interesado en categorizar los consumidores de este tipo.\n",
    "¿con que tipo de comida se correlaciona más?\n",
    "Sistema de recomendación: ¿que se debería sugerir como compra complementaria a un ciente?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn import preprocessing, cross_validation, neighbors\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = \"Wholesale_customers_data\"\n",
    "format = pd.read_csv(dataset_name + '-format.csv')\n",
    "\n",
    "df = pd.read_csv(dataset_name + '.csv')\n",
    "df['Class'] = (df['Delicatessen'] > 2000).astype(int)\n",
    "df = df.drop('Delicatessen',axis=1)\n",
    "labels = {'Usual Consumer': 0, 'Great Consumer': 1}\n",
    "\n",
    "fmts = []\n",
    "for c, d in {n: {format['field'][i]: float(vv) for i, vv in list(enumerate(v))}\n",
    "             for n, v in dict(format.drop('field', axis=1).drop('Delicatessen', axis=1)).items()}.items():\n",
    "    d.update(dict(title=c))\n",
    "    fmts.append(d)\n",
    "\n",
    "field_names = [field for field in df.drop(['Class'], 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['Class'], 1), dtype=float)\n",
    "y = np.array(df['Class'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler()\n",
    "scalery = StandardScaler()\n",
    "Xn = np.apply_along_axis(scalerX.fit_transform,0,X)\n",
    "yn = np.apply_along_axis(scalery.fit_transform,0,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "Xt = pca.transform(X)\n",
    "xx1 = Xt[y == labels['Usual Consumer']][:, 0]\n",
    "yy1 = Xt[y == labels['Usual Consumer']][:, 1]\n",
    "\n",
    "xx2 = Xt[y == labels['Great Consumer']][:, 0]\n",
    "yy2 = Xt[y == labels['Great Consumer']][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "usual_ones = Xt[y == labels['Usual Consumer']]\n",
    "great_ones = Xt[y == labels['Great Consumer']]\n",
    "\n",
    "c_min = 2\n",
    "c_max = 20\n",
    "metric = 'euclidean'\n",
    "\n",
    "silh = []\n",
    "km_model = [0] * c_max\n",
    "for t in range(2, 20):\n",
    "    km_model[t - c_min] = KMeans(n_clusters=t).fit(great_ones)\n",
    "    silh.append(silhouette_score(great_ones, km_model[t - c_min].labels_, metric=metric))\n",
    "\n",
    "idx_great = np.argmax(silh)\n",
    "n_clusters_great = idx_great + c_min\n",
    "cluster_lbl_great = km_model[idx_great].labels_\n",
    "centroid_great = {}\n",
    "for each in np.unique(cluster_lbl_great).tolist():\n",
    "    centroid_great[each] = np.mean(great_ones[cluster_lbl_great == each], 0)\n",
    "    \n",
    "silh = []\n",
    "km_model = [0] * c_max\n",
    "for t in range(2, 20):\n",
    "    km_model[t - c_min] = KMeans(n_clusters=t).fit(usual_ones)\n",
    "    silh.append(silhouette_score(usual_ones, km_model[t - c_min].labels_, metric=metric))\n",
    "\n",
    "idx_usual = np.argmax(silh)\n",
    "n_clusters_usual = idx_usual + c_min\n",
    "cluster_lbl_usual = km_model[idx_usual].labels_\n",
    "centroid_usual = {}\n",
    "for each in np.unique(cluster_lbl_usual).tolist():\n",
    "    centroid_usual[each] = np.mean(usual_ones[cluster_lbl_usual == each], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'components_': array([[  4.85437338e-06,  -3.29602341e-06,  -9.81846574e-01,\n",
       "          -1.05527899e-01,  -4.08588147e-02,  -1.51331852e-01,\n",
       "           1.64436935e-02],\n",
       "        [ -2.42258694e-05,  -6.59431479e-07,   7.91577778e-02,\n",
       "          -5.18815161e-01,  -7.68137246e-01,   1.57810130e-02,\n",
       "          -3.66443726e-01]]),\n",
       " 'copy': True,\n",
       " 'explained_variance_': array([  1.63906785e+08,   1.44654419e+08]),\n",
       " 'explained_variance_ratio_': array([ 0.46798756,  0.4130181 ]),\n",
       " 'mean_': array([  1.32272727e+00,   2.54318182e+00,   1.20002977e+04,\n",
       "          5.79626591e+03,   7.95127727e+03,   3.07193182e+03,\n",
       "          2.88149318e+03]),\n",
       " 'n_components': 2,\n",
       " 'n_components_': 2,\n",
       " 'n_samples_': 440L,\n",
       " 'noise_variance_': 8335255.5136779323,\n",
       " 'whiten': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: array([ 5945.95571666,  4444.14677679]),\n",
       "  1: array([-14014.14220861,   5894.86775211]),\n",
       "  2: array([  5950.77586828, -16573.00237945])},\n",
       " {0: array([-4906.72430097,  -963.14229213]),\n",
       "  1: array([-24643.91878405, -43214.70961087])})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_usual, centroid_great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARIOS CLASIFICADORES CON CROSSFOLDER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -VARIOS CLASIFICADORES SVC, RF, KNN, LOG_REG ----------------------------\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#  definimos la funcion de cross validation\n",
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    # Construimos el objeto kfolds \n",
    "    kf = KFold(len(y),n_folds=5,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    \n",
    "    # tenemos  que iterar sobre todos los kfolds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        \n",
    "        \n",
    "        # Iniciamos el clasificador con los keywords arguments...\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  Confianzas de los calsificadores: \n",
      " \n",
      " \n",
      "Confianza de SVC =  0.781818181818\n",
      "MATRIZ DE CONFUSION de SVC \n",
      " [[344   0]\n",
      " [ 96   0]] \n",
      "\n",
      "Confianza de KNN =  0.754545454545\n",
      "MATRIZ DE CONFUSION de KNN \n",
      " [[309  35]\n",
      " [ 73  23]] \n",
      "\n",
      "Confianza de  Log. Regression =  0.809090909091\n",
      "MATRIZ DE CONFUSION de Log. Regression \n",
      " [[337   7]\n",
      " [ 77  19]] \n",
      "\n",
      "Confianza de Random forest: 0.745454545455\n",
      "MATRIZ DE CONFUSION de RANDOM FOREST \n",
      " [[307  37]\n",
      " [ 75  21]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression as LOG_REG\n",
    "\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# ------- CONFIANZA, RECALL, PRECISION, ETC. ---------------\n",
    "print(\"--------  Confianzas de los calsificadores: \\n \\n \")\n",
    "pred_SVC = run_cv(X,y,SVC)\n",
    "print(\"Confianza de SVC = \", accuracy(y, pred_SVC))\n",
    "print(\"MATRIZ DE CONFUSION de SVC \\n\", confusion_matrix(y, pred_SVC), \"\\n\")\n",
    "\n",
    "pred_KNN = run_cv(X,y,KNN)\n",
    "print(\"Confianza de KNN = \", accuracy(y, pred_KNN))\n",
    "print(\"MATRIZ DE CONFUSION de KNN \\n\", confusion_matrix(y, pred_KNN), \"\\n\")\n",
    "\n",
    "pred_LOG_REG = run_cv(X,y,LOG_REG)\n",
    "print(\"Confianza de  Log. Regression = \", accuracy(y, pred_LOG_REG))\n",
    "print(\"MATRIZ DE CONFUSION de Log. Regression \\n\", confusion_matrix(y, pred_LOG_REG), \"\\n\")\n",
    "\n",
    "#print \"%.3f\" % accuracy(y, run_cv(X,y,RF))\n",
    "pred_RF = run_cv(X,y,RF)\n",
    "print(\"Confianza de Random forest:\", accuracy(y, pred_RF))\n",
    "print(\"MATRIZ DE CONFUSION de RANDOM FOREST \\n\", confusion_matrix(y, pred_RF), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos los limites de los SLIDES de BOKEH para que estén derentro del margen de los valores del data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  3 55  3 25  3]\n",
      "[     2      3 112151  73498  92780  60869  40827]\n"
     ]
    }
   ],
   "source": [
    "print(np.amin(X,axis=0))\n",
    "print(np.amax(X,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RECORREMOS AHORA TODO EL DATAFRAME DE FORMA QUE SEA APLICABLE A CUALQUIERA DE LOS DATA FRAMES DE LOS CASOS... \n",
    "\n",
    "lista_minimos_col = []\n",
    "lista_maximos_col = []\n",
    "\n",
    "# recorremos las columnas del DATA FRAME\n",
    "for i in range (df.shape[1]-1):                  # EL -1 es porque exxluyo el ultimo campo de la categoria o label \n",
    "    #print df.iloc[1][i:1]\n",
    "    #print df.values[1][i]\n",
    "    # añadimos el MIN y MAXde esa columna del dataframe\n",
    "    lista_minimos_col.append(df.ix[df[df.columns[i]].idxmin()][df.columns[i]])\n",
    "    lista_maximos_col.append(df.ix[df[df.columns[i]].idxmax()][df.columns[i]])\n",
    "\n",
    "print lista_minimos_col\n",
    "print lista_maximos_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graficas scattered tomadas de https://www.mapr.com/blog/predicting-airbnb-listing-prices-scikit-learn-and-apache-spark  \n",
    "import matplotlib.pyplot as plt\n",
    "#scattercols = ['price','accommodates', 'number_of_reviews', 'reviews_per_month', 'beds', 'availability_30', 'review_scores_rating']\n",
    "scattercols = df.columns[7:]\n",
    "axs = pd.scatter_matrix(df[scattercols],\n",
    "                        figsize=(12, 12), c='red')\n",
    "\n",
    "plt.figure(figsize=(1920,1080))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora calculamos los puntos cercanos a un punto dado para calcular donde están mas proximas los puntos de categoria diferente, (cancerosas). Esto nos ayudará a seleccionar las direcciones más relevantes a la hora de estudiar la evolución del punto en cuestión.\n",
    "##UTLIZAMOS UN EJEMPLO SENCILLO DE DATAFRAME EN SU LUGAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[y==2].shape\n",
    "print X[y==4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = {}\n",
    "for each in np.unique(y).tolist():\n",
    "    classes[each] = X[y==each]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = {}\n",
    "for each in np.unique(y).tolist():\n",
    "    classes[each] = X[y==each]\n",
    "\n",
    "benign = classes[labels['benign']]\n",
    "malignant = classes[labels['malignant']]\n",
    "\n",
    "b_dims = benign.shape\n",
    "m_dims = malignant.shape\n",
    "\n",
    "benign = np.reshape(benign, b_dims + (1,))\n",
    "malignant = np.reshape(malignant, m_dims + (1,))\n",
    "\n",
    "np.tile(benign, [1, 1, m_dims[0]]) - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICCIONES CON INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vamos a hacer predicciones :\n",
    "\n",
    "example_measures = np.array([[10,5,5,3,6,7,7,10,1]])\n",
    "example_measures = example_measures.reshape(len(example_measures), -1)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 PARTE: gráficas-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# condiciones que definen nuevos datasets\n",
    "df_tumoral = df[df.Class > 3]\n",
    "df_NO_tumoral = df[df.Class < 3]\n",
    "\n",
    "df_tumoral.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# graficas scattered tomadas de https://www.mapr.com/blog/predicting-airbnb-listing-prices-scikit-learn-and-apache-spark  \n",
    "import matplotlib.pyplot as plt\n",
    "#scattercols = ['price','accommodates', 'number_of_reviews', 'reviews_per_month', 'beds', 'availability_30', 'review_scores_rating']\n",
    "scattercols = df.columns\n",
    "axs = pd.scatter_matrix(df[scattercols],\n",
    "                        figsize=(12, 12), c='red')\n",
    "\n",
    "plt.figure(figsize=(1920,1080))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definimos una funcion distancia de un registro del data se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_NO_tumoral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = df_tumoral.Clump_Thickness\n",
    "y = df_tumoral.Uniform_Cell_Shape\n",
    "z = df_tumoral.Unifom_Cell_Size\n",
    "\n",
    "x2 = df_NO_tumoral.Clump_Thickness\n",
    "y2 = df_NO_tumoral.Uniform_Cell_Shape\n",
    "z2 = df_NO_tumoral.Unifom_Cell_Size\n",
    "\n",
    "\n",
    "ax1.scatter(x, y, z, c='g', marker='o')\n",
    "ax1.scatter(x2, y2, z2, c ='r', marker='o')\n",
    "\n",
    "ax1.set_xlabel('Espesor')\n",
    "ax1.set_ylabel('Forma Celular')\n",
    "ax1.set_zlabel(u'Tamaño Celular')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imprimimos las 3 primeras variables de prueba\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "x = df_tumoral.eval(df_tumoral.columns[1])\n",
    "y =  df_tumoral.eval(df_tumoral.columns[2])\n",
    "z =  df_tumoral.eval(df_tumoral.columns[3])\n",
    "\n",
    "x2 = df_NO_tumoral.eval(df_NO_tumoral.columns[1])\n",
    "y2 = df_NO_tumoral.eval(df_NO_tumoral.columns[2])\n",
    "z2 = df_NO_tumoral.eval(df_NO_tumoral.columns[3])\n",
    "\n",
    "\n",
    "ax1.scatter(x, y, z, c='g', marker='o')\n",
    "ax1.scatter(x2, y2, z2, c ='r', marker='o')\n",
    "\n",
    "ax1.set_xlabel('Espesor')\n",
    "ax1.set_ylabel('Forma Celular')\n",
    "ax1.set_zlabel(u'Tamaño Celular')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.columns[9])\n",
    "print(len(df.columns))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# funcion GENERICA QUE DIBUJA UNA COMBINACION particulr de las variable\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "def dibuja_grafica(i,j,k):\n",
    "\n",
    "    style.use('ggplot')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "    x = df_tumoral.eval(df_tumoral.columns[i])\n",
    "    y =  df_tumoral.eval(df_tumoral.columns[j])\n",
    "    z =  df_tumoral.eval(df_tumoral.columns[k])\n",
    "\n",
    "    x2 = df_NO_tumoral.eval(df_NO_tumoral.columns[i])\n",
    "    y2 = df_NO_tumoral.eval(df_NO_tumoral.columns[j])\n",
    "    z2 = df_NO_tumoral.eval(df_NO_tumoral.columns[k])\n",
    "\n",
    "    ax1.scatter(x, y, z, c='g', label='TUMORALES', marker='o')\n",
    "    ax1.scatter(x2, y2, z2, c ='r', label='SANAS', marker='o')\n",
    "    \n",
    "    #plt.scatter(x,y, label='TUMORALES', color='g', s=25, marker=\"o\")\n",
    "    #plt.scatter(x2,y2, label='SANAS', color='r', s=25, marker=\"o\")\n",
    " \n",
    "    ax1.set_xlabel(df_tumoral.columns[i])\n",
    "    ax1.set_ylabel(df_tumoral.columns[j])\n",
    "    ax1.set_zlabel(df_tumoral.columns[k])\n",
    "    \n",
    "    plt.title('Breast Cancer Dataset Analysis               ')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()  \n",
    "    #plt.savefig('graf_'+df_tumoral.columns[i]+df_tumoral.columns[j]+df_tumoral.columns[k]+'.png')\n",
    "\n",
    "dibuja_grafica(1,2,4)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# representacion de todas las GRAFICAS combinacion total de variables\n",
    "\n",
    "for counter1 in range(0,len(df.columns)-4):\n",
    "    for counter2 in range(counter1 + 1,len(df.columns)):\n",
    "        for counter3 in range(counter2 + 1,len(df.columns)):\n",
    "            try:\n",
    "                dibuja_grafica(counter1,counter2,counter3)\n",
    "                #print counter1,\",\", counter2,\",\", counter3\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recordatorio de graficas en 2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1,2,3,4,5,6,7,8]\n",
    "y = [5,2,4,2,1,4,5,2]\n",
    "\n",
    "x2 = [2,3,3,7,5,2,5]\n",
    "y2 = [3,1,0,2,4,5,2]\n",
    "\n",
    "\n",
    "#style.use('ggplot')\n",
    "\n",
    "fig = plt.figure() \n",
    "\n",
    "# AÑADIMOS UN SUBPLOT sobre fig\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(x, y, c='g', marker='o')\n",
    "ax1.scatter(x2, y2, c ='r', marker='o')\n",
    "\n",
    "plt.scatter(x,y, label='TUMORALES', color='g', s=25, marker=\"o\")\n",
    "plt.scatter(x2,y2, label='NORMALES', color='r', s=25, marker=\"o\")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Breast Cancer Analysis')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRAFICA EN 2D  con la clase como Z\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "def dibuja_grafica2D(i,j,k):\n",
    "\n",
    "    style.use('ggplot')\n",
    "    \n",
    "    x = df_tumoral.eval(df_tumoral.columns[i])\n",
    "    y =  df_tumoral.eval(df_tumoral.columns[j])\n",
    "\n",
    "    x2 = df_NO_tumoral.eval(df_NO_tumoral.columns[i])\n",
    "    y2 = df_NO_tumoral.eval(df_NO_tumoral.columns[j])\n",
    "    \n",
    "    fig = plt.figure() \n",
    "\n",
    "    # AÑADIMOS UN SUBPLOT sobre fig\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    ax1.scatter(x, y, c='g', marker='o')\n",
    "    ax1.scatter(x2, y2, c ='r', marker='o')\n",
    "\n",
    "    plt.scatter(x,y, label='TUMORALES', color='g', s=25, marker=\"o\")\n",
    "    plt.scatter(x2,y2, label='SANAS', color='r', s=25, marker=\"o\")\n",
    "\n",
    "    # ax1.scatter(x, y, z, c='g', marker='o')\n",
    "    # ax1.scatter(x2, y2, z2, c ='r', marker='o')\n",
    "\n",
    "    ax1.set_xlabel(df_tumoral.columns[i])\n",
    "    ax1.set_ylabel(df_tumoral.columns[j])\n",
    "\n",
    "\n",
    "    plt.title('Breast Cancer Dataset Analysis - Human Forecast')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    plt.savefig('graficos/graf_'+df_tumoral.columns[i]+df_tumoral.columns[j]+'.png')\n",
    "\n",
    "\n",
    "dibuja_grafica2D(1,2,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pero ahora solo estamos interesados en combinar dos a dos \n",
    "# con z que sea la clase de tumor que es el indice 9\n",
    "\n",
    "for counter1 in range(0,len(df.columns)-2):\n",
    "    for counter2 in range(counter1 + 1,len(df.columns)-1):\n",
    "            try:\n",
    "                dibuja_grafica2D(counter1,counter2,9)\n",
    "                #print counter1,\",\", counter2\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
